{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Linear Regression With an Iterative Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[Introduction](#introduction)<br>\n",
    "[Notation](#notation)<br>\n",
    "[Gradient Descent](#gradientDescent)<br>\n",
    "[Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='introduction'></a>\n",
    "\n",
    "In the second lecture of Andrew Ng's machine learning __[lecture series](https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599)__ he talks about a learning algorithm that could approximate the line of best fit of a data set by iteratively minimizing the sum of squared distances between our model and the data set. Following is a brief explanation and implementation of the algorithm in Python. We will test our algorithm on a data set courtesy of __[Siraj Raval](https://github.com/llSourcell)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation <a id='notation'></a>\n",
    "\n",
    "__Training Data__\n",
    "\n",
    "- $(x^{(i)},y^{(i)})$\n",
    "\n",
    "Our training data consists of a set of $m$ ordered lists $(x, y)$, where $x$ and $y$ are the dependant and independant variables respectively. We denote our $i^{th}$ training example as $(x^{(i)}, y^{(i)})$.\n",
    "\n",
    "__Model (hypothesis)__\n",
    "\n",
    "- $h(x) = h_{\\theta_0,\\theta_1}(x) = \\theta_0 x + \\theta_1$ or\n",
    "- $h(x_0, x_1) = h_{\\theta_0, \\theta_1}(x_0, x_1) = \\theta_0 x_0 + \\theta_1 x_1$, where $x_1 = 1$\n",
    "\n",
    "Our model $h$ is a fuction whose input are the $x$ values of our training data, and output is an estimate of the corresponding $y$ value of a specific training example. At any given moment, our model is parameterized by the changing variables $\\theta_0,\\theta_1$. It is helpful to think or our model as this equivalent form $h_{\\theta_0,\\theta_1}(x_0, x_1) = \\theta_0 x_0 + \\theta_1 x_1$, where $x_1 = 1$ for convenience later on. You can also think of our model being generated by a model generating function $g: \\mathbb{R}^2 \\to (f:\\mathbb{R}\\to\\mathbb{R})$, where $g(\\theta_0, \\theta_1) = h_{\\theta_0, \\theta_1}(x_0, x_1)$.\n",
    "\n",
    "__Loss (Cost)__\n",
    "\n",
    "- $J(\\theta_0, \\theta_1) = \\Sigma_{i=0}^{m}(h(x^{(i)}_0, x^{(i)}_1) - y^{(i)})^2$\n",
    "\n",
    "The loss function $J$ is a measure of the performance of our model, in our case the loss function is the sum of squared distances between our model paramaterized by $\\theta_0, \\theta_1$ and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA = np.genfromtxt('./res/data.csv', delimiter=',')\n",
    "ALPHA = 0.0001/DATA.size, 0.01/DATA.size\n",
    "ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of our program, we will import the computing and plotting libraries numpy and matplotlib and then get our data from a csv file. We have a hyperparameter list ALPHA which contains the learning rates for our two parameters $\\theta_0, \\theta_1$. To choose these constants I tried running the algorithm with arbitrary learning rates until I found two numbers that had pretty good performance. We can choose to run our algorithm for any number of iterations we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent <a id='gradientDescent'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descend(params: list) -> list:\n",
    "    influence0 = lambda datum: ((params[0] * datum[0]) + (params[1] * 1) - datum[1]) * datum[0]\n",
    "    influence1 = lambda datum: ((params[0] * datum[0]) + (params[1] * 1) - datum[1]) * 1\n",
    "    step0 = lambda delta: -(ALPHA[0] * delta[0])\n",
    "    step1 = lambda delta: -(ALPHA[1] * delta[1])\n",
    "    \n",
    "    delta = [0, 0]\n",
    "    for datum in DATA:\n",
    "        delta[0] += influence0(datum)\n",
    "        delta[1] += influence1(datum)\n",
    "    \n",
    "    return [params[0] + step0(delta), params[1] + step1(delta)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule that our machine will use to learn is called gradient descent. We can start by taking a look at the derivative with respect to $\\theta_0$ of the function $f(\\theta_0) = \\theta_0^2$, $\\frac{\\partial}{\\partial\\theta_0}f(\\theta_0) = \\frac{\\partial}{\\partial\\theta_0}\\theta_0^2 = 2\\theta_0$. When we compute the derivative of $f$ at $\\theta_0=5$, we get the number $10$. Another way to interperet the result $10$ is actually as the vector $(10)$, where $10$ is the height that we would gain if we were to travel $1$ unit in the direction of the vector $(10)$, the positive direction, which also happens to be the direction of steepest ascent at $\\theta_0 = 5$ on $f$. It turns out that this result generalizes to gradients as well, for exmaple, the gradient of $g(\\theta_0,\\theta_1)=\\theta_0^2\\theta_1 + 2\\theta_1$ is $\\nabla g(\\theta_0, \\theta_1) = (\\frac{\\partial}{\\partial\\theta_0}g(\\theta_0, \\theta_1), \\frac{\\partial}{\\partial\\theta_1}g(\\theta_0,\\theta_1)) = (2\\theta_0\\theta_1, \\theta_0^2+2)$, evaluating the gradient of $g$ at point $(\\theta_0,\\theta_1) = (1,2)$ gives the vector $(4,3)$, this means that $(4, 3)$ is the direction of steepest ascent on g at $(\\theta_0, \\theta_1) = (1, 2)$.\n",
    "\n",
    "With starting parmeters $(\\theta_0, \\theta_1) = (0, 0)$ we can compute the gradient of our loss function at $(\\theta_0,\\theta_1) = (0, 0)$ and then adjust the our parameters slightly in the negative direction of the gradient. After many iterations, we will find that our parameters will be converging on a local minima of our loss function.\n",
    "\n",
    "More concretely we compute the gradient of our loss function $J(\\theta_0, \\theta_1) = \\Sigma_{i=0}^{m}(h(x^{(i)}_0, x^{(i)}_1) - y^{(y)})^2$ as follows.\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_0}J = \\frac{\\partial}{\\partial\\theta_0}\\Sigma_{i=0}^{m}(h(x^{(i)}_0, x^{(i)}_1)-y^{(i)})^2$<br>\n",
    "$\\phantom{.....}= \\Sigma_{i=0}^{m}\\frac{\\partial}{\\partial\\theta_0}(h(x^{(i)}_0, x^{(i)}_1)-y^{(i)})^2$<br>\n",
    "$\\phantom{.....}= \\Sigma_{i=0}^{m}\\frac{\\partial}{\\partial\\theta_0}(\\theta_0 x_0 + \\theta_1 x_1 - y^{(i)})^2$<br>\n",
    "$\\phantom{.....}= \\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_0$<br>\n",
    "\n",
    "Similarly\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_1}J = \\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_1$\n",
    "\n",
    "Therefore, $(\\frac{\\partial}{\\partial\\theta_0}J, \\frac{\\partial}{\\partial\\theta_1}J) = (\\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_0, \\frac{\\partial}{\\partial\\theta_1}J = \\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_1)$ is the gradient \n",
    "\n",
    "From the gradient of the loss function we can derive the following learning rule as seen in our function definition __descend__.\n",
    "\n",
    "$\\theta_0 \\Leftarrow\\theta_0 - \\alpha_0\\Sigma_{i=0}^{m}(\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_0$<br>\n",
    "$\\theta_1 \\Leftarrow \\theta_1 - \\alpha_1\\Sigma_{i=0}^{m}(\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a id='conclusions'><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1.4076x + 3.6660 estimates the line of best fit after 1000 iterations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24db64d80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run() -> None:\n",
    "    params = [0, 0]\n",
    "    for i in range(ITERATIONS):\n",
    "        params = descend(params)\n",
    "    params = list(map(lambda x: \"%.4f\"%x, params))\n",
    "    print(f'y = {params[0]}x + {params[1]} estimates the line of best fit after {ITERATIONS} iterations')\n",
    "    \n",
    "    params = list(map(lambda x: float(x), params))\n",
    "    plt.scatter([x[0] for x in DATA], [y[1] for y in DATA])\n",
    "    plt.plot([0, 100], [params[1], params[1] + params[0] * 100]) \n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the gradient descent algorithm for 1000 iterations, the results are pretty good I guess."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
