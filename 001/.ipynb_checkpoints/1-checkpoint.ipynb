{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Linear Regression With an Iterative Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[Introduction](#introduction)<br>\n",
    "[Notation](#notation)<br>\n",
    "[Gradient Descent](#gradientDescent)<br>\n",
    "[Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='introduction'></a>\n",
    "\n",
    "In the second lecture of Andrew Ng's machine learning __[lecture series](https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599)__ he talks about a learning algorithm that could approximate the line of best fit of a data set by iteratively minimizing the sum of squared distances between our model and the data set. Following is a brief explanation and implementation of the algorithm in Python. We will test our algorithm on a data set courtesy of __[Siraj Raval](https://github.com/llSourcell)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation <a id='notation'></a>\n",
    "\n",
    "__Training Data__\n",
    "\n",
    "- $(x^{(i)},y^{(i)})$\n",
    "\n",
    "Our training data consists of a set of $m$ ordered lists $(x, y)$, where $x$ and $y$ are the dependant and independant variables respectively. We denote our $i^{th}$ training example as $(x^{(i)}, y^{(i)})$.\n",
    "\n",
    "__Model (hypothesis)__\n",
    "\n",
    "- $h(x) = h_{\\theta_0,\\theta_1}(x) = \\theta_0 x + \\theta_1$ or\n",
    "- $h(x_0, x_1) = h_{\\theta_0, \\theta_1}(x_0, x_1) = \\theta_0 x_0 + \\theta_1 x_1$, where $x_1 = 1$\n",
    "\n",
    "Our model $h$ is a fuction whose input are the $x$ values of our training data, and output is an estimate of the corresponding $y$ value of a specific training example. At any given moment, our model is parameterized by the changing variables $\\theta_0,\\theta_1$. It is helpful to think or our model as this equivalent form $h_{\\theta_0,\\theta_1}(x_0, x_1) = \\theta_0 x_0 + \\theta_1 x_1$, where $x_1 = 1$ for convenience later on. You can also think of our model being generated by a model generating function $g: \\mathbb{R}^2 \\to (f:\\mathbb{R}\\to\\mathbb{R})$, where $g(\\theta_0, \\theta_1) = h_{\\theta_0, \\theta_1}(x_0, x_1)$.\n",
    "\n",
    "__Loss (Cost)__\n",
    "\n",
    "- $J(\\theta_0, \\theta_1) = \\Sigma_{i=0}^{m}(h(x^{(i)}_0, x^{(i)}_1) - y^{(i)})^2$\n",
    "\n",
    "The loss function $J$ is a measure of the performance of our model, in our case the loss function is the sum of squared distances between our model paramaterized by $\\theta_0, \\theta_1$ and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./res/data.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-32912c49f985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mDATA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./res/data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mALPHA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mITERATIONS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m             \u001b[0mfhd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1685\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    614\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    615\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ./res/data.csv not found."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA = np.genfromtxt('./data.csv', delimiter=',')\n",
    "ALPHA = 0.0001/DATA.size, 0.01/DATA.size\n",
    "ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of our program, we will import the computing and plotting libraries numpy and matplotlib and then get our data from a csv file. We have a hyperparameter list ALPHA which contains the learning rates for our two parameters $\\theta_0, \\theta_1$. To choose these constants I tried running the algorithm with arbitrary learning rates until I found two numbers that had pretty good performance. We can choose to run our algorithm for any number of iterations we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent <a id='gradientDescent'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descend(params: list) -> list:\n",
    "    influence0 = lambda datum: ((params[0] * datum[0]) + (params[1] * 1) - datum[1]) * datum[0]\n",
    "    influence1 = lambda datum: ((params[0] * datum[0]) + (params[1] * 1) - datum[1]) * 1\n",
    "    step0 = lambda delta: -(ALPHA[0] * delta[0])\n",
    "    step1 = lambda delta: -(ALPHA[1] * delta[1])\n",
    "    \n",
    "    delta = [0, 0]\n",
    "    for datum in DATA:\n",
    "        delta[0] += influence0(datum)\n",
    "        delta[1] += influence1(datum)\n",
    "    \n",
    "    return [params[0] + step0(delta), params[1] + step1(delta)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule that our machine will use to learn is called gradient descent. We can start by taking a look at the derivative with respect to $\\theta_0$ of the function $f(\\theta_0) = \\theta_0^2$, $\\frac{\\partial}{\\partial\\theta_0}f(\\theta_0) = \\frac{\\partial}{\\partial\\theta_0}\\theta_0^2 = 2\\theta_0$. When we compute the derivative of $f$ at $\\theta_0=5$, we get the number $10$. Another way to interperet the result $10$ is actually as the vector $(10)$, where $10$ is the height that we would gain if we were to travel $1$ unit in the direction of the vector $(10)$, the positive direction, which also happens to be the direction of steepest ascent at $\\theta_0 = 5$ on $f$. It turns out that this result generalizes to gradients as well, for exmaple, the gradient of $g(\\theta_0,\\theta_1)=\\theta_0^2\\theta_1 + 2\\theta_1$ is $\\nabla g(\\theta_0, \\theta_1) = (\\frac{\\partial}{\\partial\\theta_0}g(\\theta_0, \\theta_1), \\frac{\\partial}{\\partial\\theta_1}g(\\theta_0,\\theta_1)) = (2\\theta_0\\theta_1, \\theta_0^2+2)$, evaluating the gradient of $g$ at point $(\\theta_0,\\theta_1) = (1,2)$ gives the vector $(4,3)$, this means that $(4, 3)$ is the direction of steepest ascent on g at $(\\theta_0, \\theta_1) = (1, 2)$.\n",
    "\n",
    "With starting parmeters $(\\theta_0, \\theta_1) = (0, 0)$ we can compute the gradient of our loss function at $(\\theta_0,\\theta_1) = (0, 0)$ and then adjust the our parameters slightly in the negative direction of the gradient. After many iterations, we will find that our parameters will be converging on a local minima of our loss function.\n",
    "\n",
    "More concretely we compute the gradient of our loss function $J(\\theta_0, \\theta_1) = \\Sigma_{i=0}^{m}(h(x^{(i)}_0, x^{(i)}_1) - y^{(y)})^2$ as follows.\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_0}J = \\frac{\\partial}{\\partial\\theta_0}\\Sigma_{i=0}^{m}(h(x^{(i)}_0, x^{(i)}_1)-y^{(i)})^2$<br>\n",
    "$\\phantom{.....}= \\Sigma_{i=0}^{m}\\frac{\\partial}{\\partial\\theta_0}(h(x^{(i)}_0, x^{(i)}_1)-y^{(i)})^2$<br>\n",
    "$\\phantom{.....}= \\Sigma_{i=0}^{m}\\frac{\\partial}{\\partial\\theta_0}(\\theta_0 x_0 + \\theta_1 x_1 - y^{(i)})^2$<br>\n",
    "$\\phantom{.....}= \\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_0$<br>\n",
    "\n",
    "Similarly\n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta_1}J = \\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_1$\n",
    "\n",
    "Therefore, $(\\frac{\\partial}{\\partial\\theta_0}J, \\frac{\\partial}{\\partial\\theta_1}J) = (\\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_0, \\frac{\\partial}{\\partial\\theta_1}J = \\Sigma_{i=0}^{m}2\\cdot (\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_1)$ is the gradient \n",
    "\n",
    "From the gradient of the loss function we can derive the following learning rule as seen in our function definition __descend__.\n",
    "\n",
    "$\\theta_0 \\Leftarrow\\theta_0 - \\alpha_0\\Sigma_{i=0}^{m}(\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_0$<br>\n",
    "$\\theta_1 \\Leftarrow \\theta_1 - \\alpha_1\\Sigma_{i=0}^{m}(\\theta^{(i)}_0 x^{(i)}_0 + \\theta^{(i)}_1 x^{(i)}_1 - y^{(i)})\\cdot x^{(i)}_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a id='conclusions'><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run() -> None:\n",
    "    params = [0, 0]\n",
    "    for i in range(ITERATIONS):\n",
    "        params = descend(params)\n",
    "    params = list(map(lambda x: \"%.4f\"%x, params))\n",
    "    print(f'y = {params[0]}x + {params[1]} estimates the line of best fit after {ITERATIONS} iterations')\n",
    "    \n",
    "    params = list(map(lambda x: float(x), params))\n",
    "    plt.scatter([x[0] for x in DATA], [y[1] for y in DATA])\n",
    "    plt.plot([0, 100], [params[1], params[1] + params[0] * 100]) \n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the gradient descent algorithm for 1000 iterations, the results are pretty good I guess."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
